{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: André Luís Silva Lopes\n",
    "\n",
    "Nome: João Pedro Gianfaldoni Andrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import random\n",
    "stopWords = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: ***[Preencha aqui o id da sua conta. Ex: @fulano ]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @JoaoAnd93050623\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'SPTrans'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode='extended').items():  \n",
    "    if (not msg.retweeted) and ('RT' not in msg.full_text) and msg.author.screen_name != \"sptrans\": \n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "#### Os tweets foram classificados em duas categorias: Relevantes e Irrelevantes.\n",
    "#### Foram considerados relevantes os tweets que criticavam a SPTrans, enquanto os irrelevantes eram todos os outros tweets que não os relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um dataframe a partir da tabela já classificada e separando em três dataframes, um dos relevantes , um dos irrelevantes, e um dos testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPTrans = pd.read_excel('SPTrans.xlsx')\n",
    "SPTrans_teste = pd.read_excel('SPTrans.xlsx', \"Teste\")\n",
    "SPTrans_I = SPTrans[SPTrans.Classificacao == \"I\"]\n",
    "SPTrans_R = SPTrans[SPTrans.Classificacao == \"R\"]\n",
    "SPTrans_R = SPTrans_R.reset_index().drop(columns = [\"index\"])\n",
    "SPTrans_I = SPTrans_I.reset_index().drop(columns = [\"index\"])\n",
    "Irrelevantes_raw = []\n",
    "Relevantes_raw = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando duas listas, uma com todos tweets relevantes e uma com todos tweets irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(SPTrans_R.Treinamento)):\n",
    "    Relevantes_raw.append(SPTrans_R.Treinamento[i])\n",
    "for i in range(0, len(SPTrans_I.Treinamento)):\n",
    "    Irrelevantes_raw.append(SPTrans_I.Treinamento[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função que limpa os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def cleanup(text):\n",
    "    \n",
    "    import string\n",
    "    punctuation = '[!-.:?;]' \n",
    "    pattern = re.compile(punctuation)\n",
    "    text = ' '.join(word for word in text.split() if not word.startswith('https'))\n",
    "    #text = ' '.join(word for word in text.split() if word.lower() not in stopWords)   (tirar stopwords diminuiu a performance do modelo)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Transformando as listas em strings limpas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relevantes_raw_txt = ', '.join(Relevantes_raw)\n",
    "Irrelevantes_raw_txt = ','.join(str(v) for v in Irrelevantes_raw)\n",
    "Relevantes = cleanup(Relevantes_raw_txt.lower())\n",
    "Irrelevantes = cleanup(Irrelevantes_raw_txt.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma serie para as palavras relevantes, uma para as palavras irrelevantes e uma com todas palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "Serie_Relevantes  = pd.Series(Relevantes.split())\n",
    "Serie_Irrelevantes  = pd.Series(Irrelevantes.split())\n",
    "Tabela_Irrelevantes = Serie_Irrelevantes.value_counts()\n",
    "Tabela_Relevantes = Serie_Relevantes.value_counts()\n",
    "Palavras = Relevantes + Irrelevantes\n",
    "Serie_Palavras = pd.Series(Palavras.split())\n",
    "Tabela_Palavras = Serie_Palavras.value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes e verificando a performance\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de Alpha e V utilizados no Laplace Smoothing, sendo V um valor \"estimado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "v = 950000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodando o classificador nos dados de teste e verificando a performance e a matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de acertos =  130\n",
      "Número de testes =  200\n",
      "Número de True-True =  32\n",
      "Número de False-False =  98\n",
      "Número de False-True =  33\n",
      "Número de True-False =  37\n",
      "Acurácia =  0.65\n"
     ]
    }
   ],
   "source": [
    "P_Relevante = 1\n",
    "P_Irrelevante = 1\n",
    "A = 0\n",
    "T = 0\n",
    "TT = 0\n",
    "FF = 0\n",
    "TF = 0\n",
    "FT = 0\n",
    "for i in range (0, len(SPTrans_teste.Teste)):\n",
    "    P_Relevante = 1\n",
    "    P_Irrelevante = 1\n",
    "    Tweet = SPTrans_teste.Teste[i]\n",
    "    Tweet = cleanup(Tweet.lower())\n",
    "    for palavra in Tweet.split():\n",
    "        if palavra in Tabela_Relevantes:\n",
    "            P_Relevante += np.log(((Tabela_Relevantes[palavra])+alpha)/(len(Tabela_Relevantes)+ alpha * v))\n",
    "        else:\n",
    "            P_Relevante += np.log(alpha/(len(Tabela_Relevantes)+ alpha * v))\n",
    "        if palavra in Tabela_Irrelevantes:\n",
    "            P_Irrelevante += np.log(((Tabela_Irrelevantes[palavra])+alpha)/(len(Tabela_Irrelevantes)+ alpha * v))\n",
    "        else:\n",
    "            P_Irrelevante += np.log(alpha/(len(Tabela_Irrelevantes)+ alpha * v))\n",
    "    if P_Relevante > P_Irrelevante and SPTrans_teste.Classificacao[i] == \"R\":\n",
    "        A+=1\n",
    "        TT+=1\n",
    "    elif P_Irrelevante > P_Relevante and SPTrans_teste.Classificacao[i] == \"I\":\n",
    "        A+=1\n",
    "        FF+=1\n",
    "    elif P_Irrelevante < P_Relevante and SPTrans_teste.Classificacao[i] == \"I\":\n",
    "        FT+=1\n",
    "    elif P_Relevante < P_Irrelevante and SPTrans_teste.Classificacao[i] == \"R\":\n",
    "        TF+=1\n",
    "\n",
    "    T+=1\n",
    "    \n",
    "print(\"Número de acertos = \", (A))\n",
    "print(\"Número de testes = \", (T))\n",
    "print(\"Número de True-True = \", (TT))\n",
    "print(\"Número de False-False = \", (FF))\n",
    "print(\"Número de False-True = \", (FT))\n",
    "print(\"Número de True-False = \", (TF))\n",
    "print(\"Acurácia = \", (A/T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o classificador com os mesmos dados utilizando a biblioteca \"SKLearn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "S = SPTrans.dropna(axis=0)\n",
    "Xt_train = S['Treinamento']\n",
    "y_train = S['Classificacao'] == 'R'\n",
    "count = CountVectorizer()\n",
    "X_train = count.fit_transform(Xt_train)\n",
    "\n",
    "model = MultinomialNB(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "S_teste = pd.read_excel('SPTrans.xlsx', \"Teste\")\n",
    "Xt_test = S_teste['Teste']\n",
    "y_test = S_teste['Classificacao'] == 'R'\n",
    "X_test = count.transform(Xt_test)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamentos e perguntas:\n",
    "\n",
    "### 1) Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "\n",
    "#### Não seria ideal utilizar o classificador construído neste projeto, pois  ele apresenta acurácia não perfeita. Ou seja, a acertividade do modelo não é muito elevada, assim prejudicando possíveis amostras geradas pelo modelo. Neste caso observariamos uma propagação do erro identificado no classificador para as amostras prejudicando os dados gerados e assim criando um ciclo que cada vez mais amplia o erro do classificador.\n",
    "\n",
    "### 2) Propor diferentes cenários para Naïve Bayes fora do contesto do projeto:\n",
    "\n",
    "#### O Teorema de Naïve Bayes é frequentemente aplicado em processamento de linguagem natural (Aplicação explorada neste projeto) e diagnósticos médicos. Essa metodologia pode ser usada quando os atributos que descrevem as variáveis forem condicionalmente independentes. \n",
    "#### Dessa forma, um problema simples e comum que exemplifica bem o teorema é o cálculo de probabilidades em cima de diagnóstico de doenças. Explorando validação de testes laboratoriais e aferição de resultados. Podendo assim, probabilísticamente, confirmar ou negar a ocorrência da doença em um determinado paciênte. Ademais, o algoritmo é muito robusto para previsões em tempo real, ainda mais por precisar de poucos dados para realizar a classificação. Entretanto, caso haja necessidade de correlacionar fatores, como do tipo causa e consequência, o Naive Bayes tende a falhar na predição. Neste caso tornando-se uma ferramenta não muito útil.\n",
    "\n",
    "\n",
    "### 3)Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "\n",
    "#### Inicialmente, poderiam ser utilizados mais tweets na etapa de treinamento, o que aumentaria o \"vocabulário\" do modelo, e tornaria mais preciso a probabilidade de cada palavra pertencer ao grupo relevante ou irrelevante. Além disso, parte da amostra de treinamento poderia ser utilizada como validação, permitindo o ajuste de parametros como os paramentros utilizados no Laplace Smoothing sem gerar overfitting ou underfitting. Além disso as palavras poderiam ser \"tratadas\" utilizando técnicas como \"Stemming\" e \"Lemmatization\" (https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "\n",
    "## Conclusão:\n",
    "\n",
    "#### Após todo andamento do projeto e sua finalização, podemos concluir algumas coisas. Primeiramente fica evidente que o modelo foi bem sucedido dentro de um espaço limitado. Em outras palavras, o modelo se mostrou eficiente dentro do espaço amostral escolhido, demonstrando uma acurácia de 65%. Concluímos que este número é aceitável, visto que utilizando uma biblioteca para Python (Sklearn) que realiza esse processo de forma mais bem estruturada; chegamos a um valor de acertividade de 66%. Assim fica validado o classificador obtido ao fim deste projeto.\n",
    "#### Podemos concluir também que provavelmente, para uma melhora na acurácia do modelo, seria interessante aumentar o número de tweets utilizados no treinamento. Desta forma o modelo teria contato com uma variedade ainda maior de construções de frases diferentes. Podendo assim ampliar \"o repertório\" alinhando melhor suas interpretações, visto que tratamos de uma aplicação de interpretação subjetiva (Linguagem Natural). Assim, com cerca de 10 mil tweets, exemplo, provável que a acurácia iria aumentar deixando o classificador mais efetivo. Podendo chegar à um classificador, \"praticamente\" ideal (\"Quanto mais tweets melhor\"). Dado que ele teve maior contato com diferentes \"subjetividades\" de escrita. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
